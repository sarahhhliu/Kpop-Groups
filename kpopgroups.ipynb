{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ishiwish/My-Projects/blob/main/kpopgroups.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "\n",
        "#Get the pdf\n",
        "pdf_path= \"Kpop_Rag_Model.pdf\"\n",
        "\n",
        "if not os.path.exists(pdf_path):\n",
        "  print(f\"[INFO]File doesn't exist...downloading\")\n",
        "\n",
        "  #enter url for pdf - Using the direct download link from Google Drive\n",
        "  url = \"https://drive.google.com/uc?export=download&id=1HaqzSoCWYEhH2OuEfS66W7MnG1pyKfzB\"\n",
        "\n",
        "  filename = pdf_path\n",
        "\n",
        "  response = requests.get(url, stream=True)\n",
        "\n",
        "  if response.status_code == 200:\n",
        "    with open(filename, 'wb') as f:\n",
        "      for chunk in response.iter_content(chunk_size=8192):\n",
        "          f.write(chunk)\n",
        "    print(f\"FILE HAS BEEN DOWNLOADED AND SAVED AS {filename}\")\n",
        "  else:\n",
        "      print(f\"Failed to download the file. Status code: {response.status_code}\")\n",
        "else:\n",
        "   print(f\"File {pdf_path} exists\")"
      ],
      "metadata": {
        "id": "2fU7GryLpr72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "# Create a text input widget for the query\n",
        "query_input = widgets.Textarea(\n",
        "    placeholder='Enter your query here...',\n",
        "    description='Query:',\n",
        "    disabled=False,\n",
        "    layout={'width': '500px', 'height': '100px'}\n",
        ")\n",
        "\n",
        "# Create an output widget to display results\n",
        "output_area = widgets.Output()\n",
        "\n",
        "# Create a button to trigger the search\n",
        "search_button = widgets.Button(\n",
        "    description='Search',\n",
        "    disabled=False,\n",
        "    button_style='', # 'success', 'info', 'warning', 'danger' or ''\n",
        "    tooltip='Click to perform search',\n",
        "    icon='search' # (FontAwesome icons available: https://fontawesome.com/icons?d=gallery&c=all)\n",
        ")\n",
        "\n",
        "# Function to handle button click\n",
        "def on_button_clicked(b):\n",
        "    with output_area:\n",
        "        output_area.clear_output()\n",
        "        query = query_input.value\n",
        "        if query:\n",
        "            print(f\"Searching for: '{query}'\\n\")\n",
        "            scores, indices = retrieve_relevant_sources(query=query,\n",
        "                                                        embeddings=embeddings,\n",
        "                                                        n_resources_to_return=3) # You can adjust the number of results here\n",
        "\n",
        "            print(\"Top relevant chunks:\")\n",
        "            for score, idx in zip(scores, indices):\n",
        "                print(f\"Score: {score:.4f}\")\n",
        "                print(\"Text:\")\n",
        "                print_wrapped(text_chunk_and_embedding_df.iloc[idx.item()][\"sentence_chunk\"])\n",
        "                print(f\"Page Number: {text_chunk_and_embedding_df.iloc[idx.item()]['page_number']}\")\n",
        "                print(\"-\" * 50) # Separator for readability\n",
        "\n",
        "# Link the button click to the function\n",
        "search_button.on_click(on_button_clicked)\n",
        "\n",
        "# Display the widgets\n",
        "display(query_input, search_button, output_area)"
      ],
      "metadata": {
        "id": "0z563zYekucR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dd17c9a"
      },
      "source": [
        "display(pages_and_texts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0646f4c"
      },
      "source": [
        "display(pages_and_texts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27b636f5"
      },
      "source": [
        "import fitz\n",
        "from tqdm.auto import tqdm\n",
        "import os\n",
        "\n",
        "def text_formatter(text:str) -> str:\n",
        "  \"\"\"Performs Minor formatting on text\"\"\"\n",
        "\n",
        "  cleaned_text = text.replace(\"\\n\", \" \").strip()\n",
        "\n",
        "  #potentially more text formatting fuctions can go here\n",
        "\n",
        "  return cleaned_text\n",
        "\n",
        "def open_and_read_pdf(pdf_path: str) -> list[dict]:\n",
        "  if not os.path.exists(pdf_path):\n",
        "    print(f\"Error: File not found at {pdf_path}\")\n",
        "    return []\n",
        "\n",
        "  doc = fitz.open(pdf_path)\n",
        "  if not doc:\n",
        "    print(f\"Error: Could not open PDF file at {pdf_path}\")\n",
        "    return []\n",
        "\n",
        "  print(f\"Successfully opened PDF with {doc.page_count} pages.\")\n",
        "  pages_and_texts = []\n",
        "  for page_number, page in tqdm(enumerate(doc)):\n",
        "    text = page.get_text()\n",
        "    text = text_formatter(text = text)\n",
        "    pages_and_texts.append({\"page_number\": page_number - 1,\n",
        "                           \"page_char_count\": len(text),\n",
        "                           \"page_word_count\": len(text.split(\" \")),\n",
        "                           \"page_sentence_count_raw\": len(text.split(\". \")),\n",
        "                           \"page_token_count\": len(text)/ 4,\n",
        "                           \"text\": text}\n",
        "                           )\n",
        "  return pages_and_texts\n",
        "\n",
        "pages_and_texts = open_and_read_pdf(pdf_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "WE GOT A PDF LETS OPEN IT"
      ],
      "metadata": {
        "id": "0XtHgGhz1d-c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  !pip install --upgrade pymupdf"
      ],
      "metadata": {
        "id": "0-JBEgcx8uk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz\n",
        "from tqdm.auto import tqdm\n",
        "import os\n",
        "\n",
        "def text_formatter(text:str) -> str:\n",
        "  \"\"\"Performs Minor formatting on text\"\"\"\n",
        "\n",
        "  cleaned_text = text.replace(\"\\n\", \" \").strip()\n",
        "\n",
        "  #potentially more text formatting fuctions can go here\n",
        "\n",
        "  return cleaned_text\n",
        "\n",
        "def open_and_read_pdf(pdf_path: str) -> list[dict]:\n",
        "  if not os.path.exists(pdf_path):\n",
        "    print(f\"Error: File not found at {pdf_path}\")\n",
        "    return []\n",
        "\n",
        "  doc = fitz.open(pdf_path)\n",
        "  if not doc:\n",
        "    print(f\"Error: Could not open PDF file at {pdf_path}\")\n",
        "    return []\n",
        "\n",
        "  print(f\"Successfully opened PDF with {doc.page_count} pages.\")\n",
        "  pages_and_texts = []\n",
        "  for page_number, page in tqdm(enumerate(doc)):\n",
        "    text = page.get_text()\n",
        "    text = text_formatter(text = text)\n",
        "    pages_and_texts.append({\"page_number\": page_number - 1,\n",
        "                           \"page_char_count\": len(text),\n",
        "                           \"page_word_count\": len(text.split(\" \")),\n",
        "                           \"page_sentence_count_raw\": len(text.split(\". \")),\n",
        "                           \"page_token_count\": len(text)/ 4,\n",
        "                           \"text\": text}\n",
        "                           )\n",
        "  return pages_and_texts\n",
        "\n",
        "pages_and_texts = open_and_read_pdf(pdf_path)\n"
      ],
      "metadata": {
        "id": "4HyBLPXT1gVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "random.sample(pages_and_texts, 1)"
      ],
      "metadata": {
        "id": "82m475ZBG2q6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame(pages_and_texts)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "9oBkJaX7HQuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe().round(2)"
      ],
      "metadata": {
        "id": "7BSqZTIhHZmk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.lang.en import English\n",
        "\n",
        "nlp = English()\n",
        "\n",
        "nlp.add_pipe(\"sentencizer\")\n",
        "\n",
        "doc = nlp(\"This is a sentence. This is another sentence. I like Kpop\")\n",
        "\n",
        "assert len(list(doc.sents)) == 3\n",
        "\n",
        "list(doc.sents)"
      ],
      "metadata": {
        "id": "K9jurOtnJWq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pages_and_texts[0]"
      ],
      "metadata": {
        "id": "v2UqJT9XMSb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wWisl9skMX2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cf066717"
      },
      "source": [
        "import fitz\n",
        "\n",
        "pdf_path = \"Kpop_Corrected_Groups_and_Soloists.pdf\" # Make sure this is the correct path to your PDF\n",
        "\n",
        "try:\n",
        "    doc = fitz.open(pdf_path)\n",
        "    if not doc:\n",
        "        print(f\"Error: Could not open PDF file at {pdf_path}\")\n",
        "    else:\n",
        "        print(f\"Successfully opened PDF with {doc.page_count} pages.\")\n",
        "        for page_number in range(doc.page_count):\n",
        "            page = doc.load_page(page_number)\n",
        "            text_blocks = page.get_text(\"blocks\") # Get text blocks\n",
        "\n",
        "            print(f\"\\n--- Page {page_number + 1} ---\")\n",
        "            if text_blocks:\n",
        "                # Print the first few text blocks to inspect\n",
        "                for i, block in enumerate(text_blocks[:5]): # Displaying up to 5 blocks\n",
        "                    print(f\"Block {i}: {block[4]}\") # block[4] contains the text\n",
        "            else:\n",
        "                print(\"No text blocks found on this page.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: File not found at {pdf_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ad4a9bb"
      },
      "source": [
        "display(pages_and_texts[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87bc4f76"
      },
      "source": [
        "display(pages_and_texts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for item in tqdm(pages_and_texts):\n",
        "  item[\"sentences\"] = list(nlp(item[\"text\"]).sents)\n",
        "  item[\"sentences\"] = [str(sent) for sent in item[\"sentences\"]]\n",
        "  item[\"page_sentence_count_spacy\"] = len(item[\"sentences\"])"
      ],
      "metadata": {
        "id": "T-e17XSsRRST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random.sample(pages_and_texts, k=1)"
      ],
      "metadata": {
        "id": "AQC4zO-3Rwu9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60220a12"
      },
      "source": [
        "Now that the text is in chunks, we can embed each chunk. Embedding is the process of converting text into numerical representations (vectors) that capture their semantic meaning. These embeddings can then be used for various downstream tasks like semantic search, clustering, or as input for other machine learning models.\n",
        "\n",
        "We will use a Sentence Transformer model from the `sentence_transformers` library to create the embeddings. If you don't have it installed, the following cell will install it."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(pages_and_texts)\n",
        "df.describe().round(2)"
      ],
      "metadata": {
        "id": "L5Vm0LIySTBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_sentence_chunk_size = 5\n",
        "\n",
        "def split_list(input_list: list[str],\n",
        "               slice_size: int=num_sentence_chunk_size) -> list[list[str]]:\n",
        "  \"\"\"Splits a list into sublists of a specified size.\"\"\"\n",
        "\n",
        "  return [input_list[i:i+slice_size] for i in range(0, len(input_list), slice_size)]\n",
        "\n",
        "test_list = list(range(25))\n",
        "test_chunks = split_list(test_list)\n",
        "print(test_chunks)"
      ],
      "metadata": {
        "id": "obg-OcwySfDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c97bddcc"
      },
      "source": [
        "!pip install sentence-transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for item in tqdm(pages_and_texts):\n",
        "  item[\"sentence_chunks\"] = split_list(input_list=item[\"sentences\"],\n",
        "                                       slice_size=num_sentence_chunk_size)\n",
        "\n",
        "item[\"num_chunks\"]= len(item[\"sentence_chunks\"])"
      ],
      "metadata": {
        "id": "b379HhVBWwdw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random.sample(pages_and_texts, k=1)"
      ],
      "metadata": {
        "id": "ne5UoFJ8XVLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(pages_and_texts)\n",
        "df.describe().round(2)"
      ],
      "metadata": {
        "id": "Lno1Nn0XXvxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from posixpath import join\n",
        "import re\n",
        "\n",
        "pages_and_chunks = []\n",
        "for item in tqdm(pages_and_texts):\n",
        "  for sentence_chunk in item[\"sentence_chunks\"]:\n",
        "   chunk_dict = {}\n",
        "   chunk_dict[\"page_number\"] = item[\"page_number\"]\n",
        "\n",
        "\n",
        "   joined_sentence_chunk = \"\". join(sentence_chunk).replace(\" \", \" \").strip()\n",
        "   joined_sentence_chunk = re.sub(r'\\. ([A-Z])', r'. \\1', joined_sentence_chunk)\n",
        "\n",
        "   chunk_dict[\"sentence_chunk\"] = joined_sentence_chunk\n",
        "\n",
        "   chunk_dict[\"chunk_char_count\"] = len(joined_sentence_chunk)\n",
        "   chunk_dict[\"chunk_word_count\"] = len([word for word in joined_sentence_chunk.split(\" \")])\n",
        "   chunk_dict[\"chunk_token_count\"] = len(joined_sentence_chunk) / 4\n",
        "\n",
        "\n",
        "   pages_and_chunks.append(chunk_dict)\n",
        "\n",
        "len(pages_and_chunks)"
      ],
      "metadata": {
        "id": "JtyGABw_X0Nz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random.sample(pages_and_chunks, k=1)"
      ],
      "metadata": {
        "id": "wBgciKlRaV5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(pages_and_chunks)\n",
        "df.describe().round(2)"
      ],
      "metadata": {
        "id": "624CjYmxallT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "min_token_length = 30\n",
        "# Sample up to 5 rows from the filtered DataFrame\n",
        "sampled_rows = df[df[\"chunk_token_count\"] <= min_token_length].sample(min(5, len(df[df[\"chunk_token_count\"] <= min_token_length])))\n",
        "\n",
        "for index, row in sampled_rows.iterrows():\n",
        "  print(f'Chunk token count:{row[\"chunk_token_count\"]}| Text: {row[\"sentence_chunk\"]}')"
      ],
      "metadata": {
        "id": "rfAEFnppc8W2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pages_and_chunks_over_min_token_len = df[df[\"chunk_token_count\"] > min_token_length].to_dict(orient=\"records\")\n",
        "pages_and_chunks_over_min_token_len[:2]"
      ],
      "metadata": {
        "id": "Xfur7duUfCzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random.sample(pages_and_chunks_over_min_token_len, k=1)"
      ],
      "metadata": {
        "id": "ep1AylcSkOSe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "embedding_model = SentenceTransformer(model_name_or_path=\"all-mpnet-base-v2\",\n",
        "                                      device = \"cpu\")\n",
        "\n",
        "sentences = [\"The sentence transformer library allows for an easy way to create embeddings.\",\n",
        "\"Sentences can embedded one by one or in a list.\",\n",
        "\"I like Kpop!\"]\n",
        "\n",
        "embeddings = embedding_model.encode(sentences)\n",
        "embeddings_dict = dict(zip(sentences, embeddings))\n",
        "\n",
        "for sentence, embedding in embeddings_dict.items():\n",
        "  print(f\"Sentence: {sentence}\")\n",
        "  print(f\"Embedding: {embedding}\")\n",
        "  print(\"\")"
      ],
      "metadata": {
        "id": "c_hcaRwukTEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(embeddings[0].shape)"
      ],
      "metadata": {
        "id": "7zlAR3TOqwhu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "for item in tqdm(pages_and_chunks_over_min_token_len):\n",
        "  item[\"embedding\"] = embedding_model.encode(item[\"sentence_chunk\"])"
      ],
      "metadata": {
        "id": "PSJ0HRbRrnLZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "de3a9e9d"
      },
      "source": [
        "print(embedding_model.device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "text_chunks = [item[\"sentence_chunk\"] for item in pages_and_chunks_over_min_token_len]\n",
        "# text_chunks[419]"
      ],
      "metadata": {
        "id": "8-4sT2i1r-Dz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ab1ca37d"
      },
      "source": [
        "print(len(text_chunks))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "text_chunk_embeddings = embedding_model.encode(text_chunks,\n",
        "                                               batch=32,\n",
        "                                               convert_to_tensor=True)\n",
        "text_chunk_embeddings"
      ],
      "metadata": {
        "id": "tKVAR9Ds1qp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pages_and_chunks_over_min_token_len[1]"
      ],
      "metadata": {
        "id": "69n1L-tw2lbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_chunks_and_embeddings = pd.DataFrame(pages_and_chunks_over_min_token_len)\n",
        "embeddings_df_save_path = \"text_chunks_and_embedding_df.csv\"\n",
        "text_chunks_and_embeddings.to_csv(embeddings_df_save_path, index=False)"
      ],
      "metadata": {
        "id": "qzO3L9vg3D_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_chunks_and_embeddings_df_load = pd.read_csv(embeddings_df_save_path)\n",
        "text_chunks_and_embeddings_df_load.head()"
      ],
      "metadata": {
        "id": "uxMNeNof4D_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "text_chunk_and_embedding_df = pd.read_csv(\"text_chunks_and_embedding_df.csv\")\n",
        "# pages_and_chunks = text_chunks_and_embeddings_df_load.to_dict(orient=\"records\")\n",
        "\n",
        "text_chunk_and_embedding_df['embedding'] = text_chunk_and_embedding_df[\"embedding\"].apply(lambda x: np.fromstring(x.strip(\"[]\"), sep= \" \"))\n",
        "embeddings = torch.tensor(np.stack(text_chunk_and_embedding_df[\"embedding\"].tolist(), axis=0),dtype=torch.float32).to(device)\n",
        "\n",
        "text_chunk_and_embedding_df"
      ],
      "metadata": {
        "id": "n6Nm7eUD4QlC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings.shape"
      ],
      "metadata": {
        "id": "Z4l3G0UlEHe9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import util, SentenceTransformer\n",
        "\n",
        "embedding_model = SentenceTransformer(model_name_or_path=\"all-mpnet-base-v2\",\n",
        "                                      device = device)"
      ],
      "metadata": {
        "id": "kyF4KtmcElIg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_chunk_and_embedding_df[\"embedding\"]"
      ],
      "metadata": {
        "id": "Kno_afzN9fB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings"
      ],
      "metadata": {
        "id": "VAJh3URjAntg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"tvxq\"\n",
        "print(f\"Query: {query}\")\n",
        "# Ensure query_embedding is on the same device as embeddings\n",
        "query_embedding = embedding_model.encode(query, convert_to_tensor=True).to(device)\n",
        "\n",
        "from time import perf_counter as timer\n",
        "\n",
        "start_time = timer()\n",
        "dot_scores = util.dot_score(a=query_embedding,b=embeddings)[0]\n",
        "end_time = timer()\n",
        "print(f\"[INFO] Time taken to get dot scores on {len(embeddings)} embeddings: {end_time - start_time:.5f} seconds\")\n",
        "\n",
        "top_results = torch.topk(dot_scores, k=min(5, len(embeddings)))\n",
        "display(top_results)"
      ],
      "metadata": {
        "id": "0qGlD1GiA1md"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pages_and_chunks[1]"
      ],
      "metadata": {
        "id": "BW03K4FWIiWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "larger_embeddings = torch.randn(100*embeddings.shape[0], 768).to(device)\n",
        "print(f\"Embeddings shape: {larger_embeddings.shape}\")\n",
        "\n",
        "start_time = timer()\n",
        "dot_scores = util.dot_score(a=query_embedding,b=larger_embeddings)[0]\n",
        "end_time = timer()\n",
        "\n",
        "print(f\"[INFO] Time taken to get dot scores on {len(embeddings)} embeddings: {end_time - start_time:.5f} seconds\")\n"
      ],
      "metadata": {
        "id": "mpAui2ItKkbu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "id": "2GgBq53gHJ6n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "\n",
        "def print_wrapped(text, wrap_length=80):\n",
        "  wrapped_text = textwrap.fill(text,wrap_length)\n",
        "  print(wrapped_text)"
      ],
      "metadata": {
        "id": "0kNrmlEWMHTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"enhypen\"\n",
        "print(f\"Query: '{query}'\\n\")\n",
        "\n",
        "print(\"Results:\")\n",
        "\n",
        "# Assuming 'top_results' is defined from a previous cell (e.g., using torch.topk)\n",
        "# and 'text_chunk_and_embedding_df' is your DataFrame with text chunks\n",
        "\n",
        "for score, idx in zip(top_results[0], top_results[1]):\n",
        "  print(f\"Score: {score:.4f}\")\n",
        "  print(\"Text: \")\n",
        "  # Access the sentence chunk from the DataFrame using the index\n",
        "  print_wrapped(text_chunk_and_embedding_df.iloc[idx.item()][\"sentence_chunk\"])\n",
        "  print(f\"Page Number: {text_chunk_and_embedding_df.iloc[idx.item()]['page_number']}\")\n",
        "  print(\"\\n\")\n",
        "\n",
        "  print(\"\")"
      ],
      "metadata": {
        "id": "rP4Tc6eKMqht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_relevant_sources(query: str,\n",
        "                              embeddings: torch.tensor,\n",
        "                              model: SentenceTransformer = embedding_model,\n",
        "                              n_resources_to_return: int = 5,\n",
        "                              print_time: bool = True):\n",
        "\n",
        "  query_embedding = model.encode(query, convert_to_tensor=True)\n",
        "\n",
        "  start_time = timer()\n",
        "  dot_scores = util.dot_score(a=query_embedding,b=embeddings)[0]\n",
        "  end_time = timer()\n",
        "\n",
        "\n",
        "  if print_time:\n",
        "    print(f\"[INFO] Time taken to get dot scores on {len(embeddings)} embeddings: {end_time - start_time:.5f} seconds\")\n",
        "\n",
        "  scores, indices = torch.topk(dot_scores,\n",
        "                               k=n_resources_to_return)\n",
        "\n",
        "  return scores, indices"
      ],
      "metadata": {
        "id": "IWQ1TqdVRdxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97d4915c"
      },
      "source": [
        "# Example usage of the retrieve_relevant_sources function\n",
        "query = \"what are the kpop generations?\"\n",
        "n_results = 3 # Number of relevant chunks to retrieve\n",
        "\n",
        "scores, indices = retrieve_relevant_sources(query=query,\n",
        "                                            embeddings=embeddings,\n",
        "                                            n_resources_to_return=n_results)\n",
        "\n",
        "print(f\"Query: '{query}'\\n\")\n",
        "print(\"Top relevant chunks:\")\n",
        "\n",
        "# Loop through the top results and display the text and page number\n",
        "for score, idx in zip(scores, indices):\n",
        "  print(f\"Score: {score:.4f}\")\n",
        "  print(\"Text:\")\n",
        "  print_wrapped(text_chunk_and_embedding_df.iloc[idx.item()][\"sentence_chunk\"])\n",
        "  print(f\"Page Number: {text_chunk_and_embedding_df.iloc[idx.item()]['page_number']}\")\n",
        "  print(\"-\" * 50) # Separator for readability"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ccb7c8f"
      },
      "source": [
        "Now we can load an embedding model and create embeddings for each chunk."
      ]
    }
  ]
}